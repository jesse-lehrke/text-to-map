{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text locations to map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a dataframe, extracts all place names from text column, gets counts of these and then collects coordnates for them.\n",
    "\n",
    "It then maps these on a map. \n",
    "\n",
    "A category column(s) is also included, for mapping two different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT TO DO:\n",
    "\n",
    "Split this code into 1) dataframe prep and coordinate retrival and 2) actual mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this\n",
    "#https://pyrosm.readthedocs.io/en/latest/basics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual imports\n",
    "\n",
    "import pandas as pd\n",
    "import codecs\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map imports\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './DATA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframes available\n",
    "\n",
    "onlyfiles = [f for f in listdir(PATH) if isfile(join(PATH, f))]\n",
    "print(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which file to use\n",
    "\n",
    "filename = onlyfiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "\n",
    "CONVERTERS = {'pos_tokens': eval,\n",
    "            'jkeywords_clean': eval}\n",
    "\n",
    "df = pd.read_csv(PATH + filename, converters=CONVERTERS, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific POS extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the two positions of speech I need for locations/places\n",
    "\n",
    "def pos_extract(pos, pos2, txt):\n",
    "      x = [token for token in txt if token.endswith(pos) or token.endswith(pos2)]\n",
    "      y = [token.split('/')[0] for token in x] # use when I need lists with just these!\n",
    "      #return len(x)\n",
    "      return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my chinese data/jieba output = ns and nrt mostly places? #nr = people ?\n",
    "\n",
    "df['place_tags'] = df['pos_tokens'].apply(lambda x: pos_extract('nrt', 'ns', x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list and count of locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making list of place names and how often they appear\n",
    "\n",
    "count1 = pd.Series(Counter(chain.from_iterable(df['place_tags'])))\n",
    "count1 = count1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "count1.to_csv(PATH + 'places.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Start here for collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH + 'places.csv', names=['place', 'n'], header=0, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old code - use if place name and POS are extracted\n",
    "\n",
    "#df[['city', 'pos']] = df['city'].str.split('/', 1, expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geopy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but not integrated in task.\n",
    "This returns no errors for non-place names, so would need a really better filter for these or another means to detect them\n",
    "Ideal world would be nice - smoother than Selenium calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "df_slice = df[:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some other options\n",
    "# from functools import partial\n",
    "\n",
    "\n",
    "# geolocator = Nominatim(user_agent=\"specify_your_app_name_here\")\n",
    "# geocode = partial(geolocator.geocode, language=\"es\")\n",
    "\n",
    "# geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "# df['location'] = df['name'].apply(geocode)\n",
    "# df['point'] = df['location'].apply(lambda loc: tuple(loc.point) if loc else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_list = []\n",
    "long_list = []\n",
    "\n",
    "loc_list = []\n",
    "count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_slice.iterrows():\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"Test\")\n",
    "\n",
    "    location = geolocator.geocode(row['place'])\n",
    "    print(location)\n",
    "\n",
    "    loc_list.append(row['place'])\n",
    "    lat_list.append(location.latitude)\n",
    "    long_list.append(location.longitude)\n",
    "    count.append(row['n']) #n_count\n",
    "\n",
    "    time.sleep(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopy_map_df = pd.DataFrame(zip(loc_list, lat_list, long_list, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopy_map_df = geopy_map_df.rename(columns={0: 'location', 1: 'latitude',\n",
    "            2: 'longitude',\n",
    "            3: 'n_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geopy_map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect coordinates - Selenium and Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_list = []\n",
    "loc_list = []\n",
    "key_list = []\n",
    "count = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing\n",
    "df_slice = df[:1000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is Chromedriver\n",
    "# Version 129.0.6668.70\n",
    "\n",
    "driver_location = './INPUT/chromedriver' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# First seems super needed on my system\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\")\n",
    "options.add_argument(\"--disable-infobars\")\n",
    "#options.add_argument(\"--disable-extensions\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "options.add_argument(\"user-data-dir=selenium\")\n",
    "\n",
    "#options.add_argument(\"--start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(driver_location), options=options)\n",
    "\n",
    "for index, row in df_slice.iterrows():\n",
    "      keyword = str(row['place']) #name\n",
    "      #url_search = 'https://www.google.com/maps/search/?api=1&query=' + lat + ', ' + long\n",
    "      url_search = 'https://www.google.com/maps/'\n",
    "\n",
    "      driver.get(url_search)\n",
    "\n",
    "      driver.implicitly_wait(1000)\n",
    "\n",
    "      ActionChains(driver).move_to_element(test).click().send_keys(keyword).perform() # 社会信用体系\n",
    "\n",
    "      test.send_keys(Keys.RETURN)\n",
    "\n",
    "      driver.implicitly_wait(1000)\n",
    "\n",
    "      while driver.current_url == 'https://www.google.com/maps/':\n",
    "            driver.implicitly_wait(10)\n",
    "      \n",
    "      time.sleep(3)\n",
    "      driver.implicitly_wait(500)\n",
    "\n",
    "      url = driver.current_url\n",
    "\n",
    "\n",
    "      core, api = url.split('@')\n",
    "\n",
    "      try:\n",
    "            coordinates = api.split(',')[:2]\n",
    "            print(coordinates)\n",
    "            coord_list.append(coordinates)\n",
    "            key_list.append(keyword)\n",
    "            count.append(row['n']) #n_count\n",
    "      except:\n",
    "            print('Coordinate parsing failed')\n",
    "      \n",
    "      try:\n",
    "            location = core.split('place')[-1].replace('/', '')\n",
    "            print(location)\n",
    "            loc_list.append(location)\n",
    "      \n",
    "      except:\n",
    "            print('Location parsing failed')\n",
    "\n",
    "      try:\n",
    "            with codecs.open(PATH + 'running_geo_data.csv', 'a', 'utf-8',) as f:\n",
    "                  f.write(str(coordinates) + ';' + location + ';' + keyword + ';' + str(row['n']) + ';' + str(coordinates[0]) + ';' +  str(coordinates[1]) + '\\n')\n",
    "      except:\n",
    "           with codecs.open(PATH + 'running_geo_data.csv', 'a', 'utf-8',) as f:\n",
    "                 f.write(keyword + ':' + 'NONE' + ';' + 'NONE' + ';' + str(row['n']) + '\\n')\n",
    "\n",
    "      print(index)\n",
    "\n",
    "      time.sleep(4)\n",
    "      # only needed to merge from time to time\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though running data was saved, making dataframe here from running lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = pd.DataFrame(zip(coord_list, loc_list, key_list, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = map_df.rename(columns={0: 'coordinates', 1: 'location',\n",
    "            2: 'name',\n",
    "            3: 'n_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(map_df['coordinates'].to_list(), columns=['latitude', 'longitude'])\n",
    "map_df = pd.concat([map_df, temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df_update = pd.concat([map_df, keep])\n",
    "map_df_update.reset_index(inplace=True, drop=True)\n",
    "map_df_update.sort_values(by='n_count', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.to_csv(PATH + 'map_coords.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catategory (dummy variable) work to add a mapping option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will come back to clean this- basic task, not interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df[['place_tags', 'category']]\n",
    "temp_df = temp_df[temp_df.place_tags.map(len) > 0]\n",
    "\n",
    "#df[df['amp'].map(len) == 495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERTERS = {'coordinates': eval}\n",
    "map_df = pd.read_csv(PATH + 'map_coords.csv', converters=CONVERTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS this old now? Not the way I prefer to do it\n",
    "\n",
    "def get_categories_rev(x):\n",
    "      p = []\n",
    "\n",
    "      for index, row in map_df.iterrows():\n",
    "            if row['name'] in x:\n",
    "                  print(row['name'])\n",
    "                  p.append(row.coordinates)\n",
    "                  #return(row.category)\n",
    "            else:\n",
    "                  pass\n",
    "      #print(p)\n",
    "      return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_categories_rev(temp_df['place_tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['coordinates'] = temp_df['place_tags'].apply(lambda x: get_categories_rev(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {'test': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster now by about 33%\n",
    "\n",
    "def get_categories(x):\n",
    "      new_dict = {}\n",
    "      for index, row in temp_df.iterrows():\n",
    "            if x in row.place_tags:\n",
    "                  if x not in list(new_dict.keys()):\n",
    "                        new_dict[x] = []\n",
    "                  new_dict[x].append(row.category)\n",
    "                  #p.append(row.category)\n",
    "                  #return(row.category)\n",
    "            else:\n",
    "                  pass\n",
    "            # try:\n",
    "            #       print(new_dict[x])\n",
    "            # except:\n",
    "            #       pass\n",
    "      print(len(new_dict[x]))\n",
    "      return new_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df['categories'] = map_df['name'].apply(lambda x: get_categories(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.to_csv(PATH + 'map_coords.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and DF work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERTERS = {'latitude': float, 'longitude': float}\n",
    "\n",
    "map_df = pd.read_csv(PATH + 'map_coords.csv', converters=CONVERTERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only needed to merge from time to time\n",
    "\n",
    "# map_df_2 = pd.read_csv('../data/map_coords_final_DEL.csv', converters=CONVERTERS)\n",
    "# map_df = pd.concat([map_df_1, map_df_2])\n",
    "# map_df.reset_index(inplace=True, drop=True)\n",
    "# len(map_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories for map work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get counts for the categories for each location\n",
    "\n",
    "NOTE - move to first catagory section and save this, and delete old catagories column as that list is not needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERTERS = {'latitude': float, 'longitude': float, 'categories':eval}\n",
    "\n",
    "map_df = pd.read_csv(PATH + 'map_coords.csv', converters=CONVERTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_count(x):\n",
    "      count = pd.Series(Counter(x))\n",
    "      count_dict = count.to_dict()\n",
    "      return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df['cat_dict'] = map_df['categories'].apply(lambda x: row_count(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(map_df['cat_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df = pd.merge(map_df, df, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This all relates to bad locations, i.e. which return where you are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would like to find a neater way to manage this, including also filtering before we collect coordinates (to avoid uneeded requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len('https:www.google.commapssearch%E6%B7%B1%E5%BA%A6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure why I had a recollect here\n",
    "\n",
    "recollect = map_df[map_df['coordinates'] == \"['52.5225887', '13.2993244']\"]\n",
    "keep = map_df[map_df['coordinates'] != \"['52.5225887', '13.2993244']\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very rough way to drop bad locations\n",
    "map_df = map_df[map_df.location.str.len() < 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(map_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_df = pd.read_csv(PATH + 'map_coords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = []\n",
    "long = []\n",
    "n_count = []\n",
    "s_count = [] # a category count\n",
    "p_count = [] # a category count\n",
    "\n",
    "\n",
    "for index, row in map_df.iterrows():\n",
    "      if row.latitude > 60 or row.latitude < 15 or row.longitude > 135 or row.longitude < 70:\n",
    "            print('Outside China')\n",
    "            pass\n",
    "      else:\n",
    "            lat.append(row.latitude)\n",
    "            long.append(row.longitude)\n",
    "            n_count.append(row.n_count)\n",
    "            s_count.append(row.scientific)\n",
    "            p_count.append(row.political)                        \n",
    "            print(row.n_count)\n",
    "      \n",
    "      #15-60 lat\n",
    "      #70-135 lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If list is all in China or for other maps\n",
    "#lat = map_df['latitude'].to_list()\n",
    "#long = map_df['longitude'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(long,lat)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps digression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPS_PATH = '/home/jesselehrke/Documents/china_map_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to process existing csv\n",
    "mapfiles = [f for f in listdir(MAPS_PATH) if isfile(join(MAPS_PATH, f))]\n",
    "print('Index, Filename' + '\\n')\n",
    "print(list(zip([str(index) for index, value in enumerate(mapfiles)], mapfiles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map_file = 'china-latest.osm.pbf'\n",
    "river_file = mapfiles[89]\n",
    "map_file = mapfiles[93]\n",
    "land_use = mapfiles[9]\n",
    "\n",
    "#map_file = 'china-provinces.json'\n",
    "#https://www.kaggle.com/datasets/gpreda/china-regions-map?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_link = MAPS_PATH + map_file\n",
    "\n",
    "# river_link = MAPS_PATH + river_file\n",
    "# use_link = MAPS_PATH + land_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china = gpd.read_file(map_link, bbox=None, mask=None, rows=None)\n",
    "\n",
    "# rivers = gpd.read_file(river_link, bbox=None, mask=None, rows=None)\n",
    "# use = gpd.read_file(use_link, bbox=None, mask=None, rows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(gpd.plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (20,7))\n",
    "china.plot(ax = ax, facecolor=\"none\",  linewidth=0.5, edgecolor='red', alpha=.4)\n",
    "#rivers.plot(ax = ax, linewidth=0.2, alpha=.9)\n",
    "# use.plot(ax = ax, linewidth=0.2, alpha=.9)\n",
    "\n",
    "fig.suptitle('China')\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china.loc[0, 'geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back on task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = gpd.GeoDataFrame(geometry = geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df['n'] = n_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df['p'] = p_count\n",
    "geo_df['s'] = s_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = geo_df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo_df.to_csv('../data/map_coords_for_mapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo_df = pd.read_csv('../data/map_coords_for_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = []\n",
    "s1 = []\n",
    "for index, row in geo_df.iterrows():\n",
    "      if row.p > row.s:\n",
    "            p1.append(row.p * 100 / row.s)\n",
    "            s1.append(0)\n",
    "      else:\n",
    "            s1.append(row.s * 100 / row.p)\n",
    "            p1.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df['p1'] = p1\n",
    "geo_df['s1'] = s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (30,30))\n",
    "china.plot(ax = ax)\n",
    "\n",
    "g = geo_df.plot(ax = ax, markersize = geo_df['p1'], color = 'orange',marker = '.',label = 'Locations', alpha=.2)\n",
    "g = geo_df.plot(ax = ax, markersize = geo_df['s1'], color = 'green',marker = '.',label = 'Locations', alpha=.9)\n",
    "\n",
    "plt.savefig(\"./OUTPUTS/map_test.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not sure what I was doing here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged = gpd.sjoin(china, geo_df, how=\"inner\")# op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where did name_1 come from ?\n",
    "\n",
    "data_merged.groupby('name_1').agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working as name_1 not specified\n",
    "\n",
    "unit = geo_df.dissolve(by='name_1', aggfunc='sum')\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (30,30))\n",
    "china.plot(ax = ax)\n",
    "\n",
    "g = unit.plot(ax = ax, markersize = geo_df['n'], color = 'red',marker = '.',label = 'Locations', alpha=.2)\n",
    "\n",
    "plt.savefig(\"./OUTPUTS/map_total.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (30,30))\n",
    "china.plot(ax = ax)\n",
    "\n",
    "g = geo_df.plot(ax = ax, markersize = geo_df['n'], color = 'red',marker = '.',label = 'Locations', alpha=.2)\n",
    "plt.savefig(\"./OUTPUTS/map_total.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (30,30))\n",
    "china.plot(ax = ax)\n",
    "\n",
    "#g = geo_df.plot(ax = ax, markersize = geo_df['s'], color = 'orange',marker = '.',label = 'Locations', alpha=.2)\n",
    "g = geo_df.plot(ax = ax, markersize = geo_df['p'], color = 'red',marker = '.',label = 'Locations', alpha=.2)\n",
    "\n",
    "plt.savefig(\"./OUTPUTS/map_pol.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (30,30))\n",
    "china.plot(ax = ax)\n",
    "\n",
    "g = geo_df.plot(ax = ax, markersize = geo_df['s'], color = 'red',marker = '+',label = 'Locations', alpha=.4)\n",
    "g = geo_df.plot(ax = ax, markersize = geo_df['p'], color = 'green',marker = 'x',label = 'Locations', alpha=.4)\n",
    "\n",
    "plt.savefig(\"./OUTPUTS/map_sci.png\", format='png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must just be some reference code? \n",
    "\n",
    "# ward.crs = {'init':\"epsg:4326\"}\n",
    "# geo_df.crs = {'init':\"epsg:4326\"}\n",
    "\n",
    "# # plot the polygon\n",
    "# ax = ward.plot(alpha=0.35, color='#d66058', zorder=1)\n",
    "# # plot the boundary only (without fill), just uncomment\n",
    "# #ax = gpd.GeoSeries(ward.to_crs(epsg=3857)['geometry'].unary_union).boundary.plot(ax=ax, alpha=0.5, color=\"#ed2518\",zorder=2)\n",
    "# ax = gpd.GeoSeries(ward['geometry'].unary_union).boundary.plot(ax=ax, alpha=0.5, color=\"#ed2518\",zorder=2)\n",
    "\n",
    "# # plot the marker\n",
    "# ax = geo_df.plot(ax = ax, markersize = 20, color = 'red',marker = '*',label = 'Delhi', zorder=3)\n",
    "\n",
    "# ctx.add_basemap(ax, crs=geo_df.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73afed2e6e5cae272ca1e451939a06651cb7194d426a79bc157d2d09ec23572e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('sab22_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
